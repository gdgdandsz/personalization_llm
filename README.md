# Personalizing Large Language Models for Value Judgments Based on Cognitive Models

## Overview
This project explores the advancement of personalization capabilities in Large Language Models (LLMs) to better simulate human cognition and decision-making processes. By integrating techniques such as K-Means clustering, Prompt Optimization with Textual Gradients (ProTeGi), and Low-Rank Adaptation (LoRA), we aim to enhance the model's ability to produce more tailored and human-like responses.

## Project Description
Large Language Models like GPT and LLaMA have transformed natural language processing (NLP) by generating responses that often rival human performance. However, these models frequently deliver generalized responses that lack personalization and context-specific flexibility. Our project addresses these limitations by developing a novel framework that significantly improves personalized response generation in LLMs.

### Key Features
- **K-Means Clustering**: Segmenting the population based on cultural neuroscience to develop typical population archetypes for prompt customization.
- **ProTeGi**: An iterative refinement of prompts to instruct more context-aware responses.
- **LoRA**: Enabling efficient fine-tuning without the need for retraining the entire model.


## Installation
To set up the project environment:
```
git clone https://github.com/gdgdandsz/personalization_llm
cd personalization_llm
```

## Contributing
We welcome contributions to this project! Please feel free to fork the repository, make changes, and submit pull requests.

## Acknowledgements
- Prof. Hongyi Wen, for his invaluable guidance and insights.
- Our families and friends, for their unwavering support throughout this research journey.

## Contact
For any queries, please open an issue on the repository, or contact one of the contributors directly through GitHub.
